{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae69ede2-3043-4a99-bec5-aa4da90c13d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = r\"C:\\projects\\rows.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "df.info()  # Check data types\n",
    "df.head()  # View sample data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72949ead-bd63-4814-ad91-761d511e497f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Hello, how are you?\")\n",
    "tokens = [token.text for token in doc]\n",
    "\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726e4f68-97f7-4376-94c5-d255e9ad99c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = r\"C:\\projects\\rows.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "df.info()  # Check data types\n",
    "df.head()  # View sample data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8361b1c-80a4-4549-b911-ffd73d4494ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f075b38b-5486-4a61-ac17-034fb3020c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=[\"Consumer complaint narrative\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183ecd52-e386-40c5-91a1-cd9036f0b55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna({\"Company public response\": \"No response\", \n",
    "           \"Tags\": \"No tags\", \n",
    "           \"Consumer consent provided?\": \"Unknown\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e05cc7-36ab-4613-b0f2-f53cfd6378e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4514346-d9d9-46d5-8c2e-e35f5479ec5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaae0b4b-cf15-4dd7-bd00-19b43bfea8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    if pd.isnull(text):  # Handle missing values\n",
    "        return \"\"\n",
    "\n",
    "    doc = nlp(text.lower())  # Convert to lowercase & process with spaCy\n",
    "    \n",
    "    # Tokenization, removing stopwords & punctuation, and lemmatization\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]\n",
    "    \n",
    "    return \" \".join(tokens)  # Join tokens back into a string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b46cdef-339e-45f9-a240-48c391ba119b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = r\"C:\\projects\\rows.csv\"  # Ensure the correct path\n",
    "df = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "df.info()  # Check if the dataset is loaded correctly\n",
    "df.head()  # Display a few rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888ea5c5-443e-4a7f-a4aa-792ac93f71ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df.sample(5000, random_state=42)  # Load a sample of 5000 rows\n",
    "df_sample[\"cleaned_text\"] = df_sample[\"Consumer complaint narrative\"].apply(preprocess_text)\n",
    "df_sample.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fc9927-5ec1-474f-a09e-4d9d4f4f949e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda22fa5-a380-431a-9cbb-9f58cbdf2b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample[[\"Consumer complaint narrative\", \"cleaned_text\"]].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5995cc-30f9-4c9e-9e52-effe8a92b2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample[\"cleaned_text\"].isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4504a48e-fb81-4783-a035-7bfab610ddbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # Limit vocabulary size\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df_sample[\"cleaned_text\"].dropna())\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "import pandas as pd\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display first few rows\n",
    "tfidf_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7f01c7-9504-4515-abba-5695d686519b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "# Load spaCy's pre-trained model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Function to get document vector\n",
    "def get_spacy_embedding(text):\n",
    "    doc = nlp(text)\n",
    "    return doc.vector\n",
    "\n",
    "# Apply function to cleaned text\n",
    "df_sample[\"spacy_embedding\"] = df_sample[\"cleaned_text\"].dropna().apply(get_spacy_embedding)\n",
    "\n",
    "# Convert embeddings to a numpy array for further analysis\n",
    "spacy_embeddings = np.vstack(df_sample[\"spacy_embedding\"].dropna().values)\n",
    "\n",
    "# Display shape of the embeddings\n",
    "spacy_embeddings.shape  # Should be (num_samples, 300) for spaCy's 300-dimensional vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691a10d5-85e5-4cf7-afbd-80f574ccaeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Reduce TF-IDF embeddings to 2D\n",
    "pca_tfidf = PCA(n_components=2)\n",
    "tfidf_2d = pca_tfidf.fit_transform(tfidf_matrix.toarray())\n",
    "\n",
    "# Reduce spaCy embeddings to 2D\n",
    "pca_spacy = PCA(n_components=2)\n",
    "spacy_2d = pca_spacy.fit_transform(spacy_embeddings)\n",
    "\n",
    "# Plot TF-IDF embeddings\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(tfidf_2d[:, 0], tfidf_2d[:, 1], alpha=0.5)\n",
    "plt.title(\"TF-IDF Embeddings Visualized using PCA\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.show()\n",
    "\n",
    "# Plot spaCy embeddings\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(spacy_2d[:, 0], spacy_2d[:, 1], alpha=0.5, color='red')\n",
    "plt.title(\"spaCy Embeddings Visualized using PCA\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04db47f-a7d4-45f0-bbd7-9c06411b6841",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compute cosine similarity for TF-IDF vectors\n",
    "tfidf_cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# Find the most similar complaints (excluding self-similarity)\n",
    "import numpy as np\n",
    "np.fill_diagonal(tfidf_cosine_sim, 0)  # Remove self-similarity\n",
    "\n",
    "# Get the most similar pair\n",
    "most_similar_idx = np.unravel_index(np.argmax(tfidf_cosine_sim), tfidf_cosine_sim.shape)\n",
    "print(f\"Most similar complaints: {most_similar_idx}\")\n",
    "\n",
    "# Display the actual complaints\n",
    "print(\"Complaint 1:\", df_sample.iloc[most_similar_idx[0]][\"cleaned_text\"])\n",
    "print(\"Complaint 2:\", df_sample.iloc[most_similar_idx[1]][\"cleaned_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73e8736-0a7d-4e2b-a16e-f74f5eab881e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names and IDF scores\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "idf_scores = tfidf_vectorizer.idf_\n",
    "\n",
    "# Sort features by importance\n",
    "important_words = sorted(zip(feature_names, idf_scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Display the top 10 most important words\n",
    "print(\"Top 10 important words in TF-IDF:\")\n",
    "for word, score in important_words[:10]:\n",
    "    print(f\"{word}: {score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2928644d-aaf7-48c0-8e1c-709507227ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Reduce dimensionality of spaCy embeddings to 2D\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "spacy_embeddings_2d = tsne.fit_transform(spacy_embeddings)\n",
    "\n",
    "# Plot the reduced embeddings\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(spacy_embeddings_2d[:, 0], spacy_embeddings_2d[:, 1], alpha=0.5)\n",
    "plt.title(\"t-SNE Visualization of spaCy Embeddings\")\n",
    "plt.xlabel(\"Component 1\")\n",
    "plt.ylabel(\"Component 2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecc0601-3337-45b1-9d9c-c70761909128",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compute cosine similarity for spaCy embeddings\n",
    "spacy_cosine_sim = cosine_similarity(spacy_embeddings)\n",
    "\n",
    "# Find the most similar complaints (excluding self-similarity)\n",
    "np.fill_diagonal(spacy_cosine_sim, 0)  # Remove self-similarity\n",
    "most_similar_idx = np.unravel_index(np.argmax(spacy_cosine_sim), spacy_cosine_sim.shape)\n",
    "\n",
    "# Display the most similar complaints\n",
    "print(f\"Most similar complaints (spaCy embeddings): {most_similar_idx}\")\n",
    "print(f\"Complaint 1: {df_sample.iloc[most_similar_idx[0]]['cleaned_text']}\")\n",
    "print(f\"Complaint 2: {df_sample.iloc[most_similar_idx[1]]['cleaned_text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25733d5-78bc-4448-9d98-a5ef991ab911",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Set number of clusters (adjust as needed)\n",
    "num_clusters = 5\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "df_sample[\"cluster\"] = kmeans.fit_predict(spacy_embeddings)\n",
    "\n",
    "# Print cluster assignments\n",
    "df_sample[[\"cleaned_text\", \"cluster\"]].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad0674e-8afc-45d7-b74e-0e40a02045d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Get cluster labels\n",
    "clusters = df_sample[\"cluster\"].values\n",
    "\n",
    "# Compute average TF-IDF scores per cluster\n",
    "cluster_tfidf = np.zeros((num_clusters, tfidf_matrix.shape[1]))\n",
    "\n",
    "for i in range(num_clusters):\n",
    "    cluster_tfidf[i] = tfidf_matrix[clusters == i].mean(axis=0)\n",
    "\n",
    "# Get top words for each cluster\n",
    "top_n = 10  # Number of top words to display per cluster\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "for i in range(num_clusters):\n",
    "    top_words_idx = cluster_tfidf[i].argsort()[-top_n:][::-1]\n",
    "    top_words = [feature_names[j] for j in top_words_idx]\n",
    "    print(f\"Cluster {i} top words: {', '.join(top_words)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e27b67d-c07a-4561-8cb6-9c1ca202d179",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_clusters):\n",
    "    sample_text = df_sample[df_sample[\"cluster\"] == i][\"cleaned_text\"].iloc[0]\n",
    "    print(f\"Cluster {i} Example: {sample_text}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f0c08b-9cad-4e3d-8661-6b45e7289ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Use CountVectorizer for LDA (since it works better with raw word counts)\n",
    "count_vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "count_matrix = count_vectorizer.fit_transform(df_sample[\"cleaned_text\"])  \n",
    "\n",
    "# Use TfidfVectorizer for NMF (since it works better with weighted word frequencies)\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df_sample[\"cleaned_text\"])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473a17ce-7f5b-4fbe-8aa3-7100f9af5f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Set the number of topics\n",
    "num_topics = 5  \n",
    "\n",
    "# Train LDA model\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "lda.fit(count_matrix)\n",
    "\n",
    "# Get feature names\n",
    "lda_feature_names = count_vectorizer.get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a42990-bdcb-45ac-ab52-c88031ace3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Train NMF model\n",
    "nmf = NMF(n_components=num_topics, random_state=42)\n",
    "nmf.fit(tfidf_matrix)\n",
    "\n",
    "# Get feature names\n",
    "nmf_feature_names = tfidf_vectorizer.get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ad1ebc-3e97-4b41-a1fc-aedc9a6e1d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, num_words=10):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"\\nTopic {topic_idx + 1}:\")\n",
    "        print(\", \".join([feature_names[i] for i in topic.argsort()[:-num_words - 1:-1]]))\n",
    "\n",
    "print(\"\\n🔹 Topics extracted using LDA:\")\n",
    "display_topics(lda, lda_feature_names)\n",
    "\n",
    "print(\"\\n🔹 Topics extracted using NMF:\")\n",
    "display_topics(nmf, nmf_feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5d9fee-8d2f-476c-bab6-ccde9028d98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "print(\"pyLDAvis installed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3f1f2f-4e94-4969-a363-b26a73db288a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy\n",
    "print(\"Gensim & NumPy are working!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8954d6e6-33fc-4484-ada6-e5649998b1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-import necessary libraries\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Check if lda_model exists\n",
    "try:\n",
    "    print(lda_model)\n",
    "except NameError:\n",
    "    print(\"⚠️ 'lda_model' is not defined! Re-run the topic modeling code.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbb7fbd-a8c0-450b-877f-2c2e6d7282f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample: Load your text dataset (replace with actual dataset)\n",
    "text_data = [\n",
    "    \"The bank approved my mortgage, but the interest rate was too high.\",\n",
    "    \"I had issues with my credit card payment and customer service was unhelpful.\"\n",
    "]\n",
    "\n",
    "# Process text: Tokenization, lemmatization, and stopword removal\n",
    "processed_texts = []\n",
    "for doc in nlp.pipe(text_data):\n",
    "    tokens = [token.lemma_.lower() for token in doc if not token.is_stop and token.is_alpha]\n",
    "    processed_texts.append(tokens)\n",
    "\n",
    "print(\"Processed Texts:\", processed_texts)  # Check the output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54558562-52bb-41a1-a483-0418d448ebc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "# Create dictionary\n",
    "dictionary = corpora.Dictionary(processed_texts)\n",
    "\n",
    "# Convert to bag-of-words corpus\n",
    "corpus = [dictionary.doc2bow(text) for text in processed_texts]\n",
    "\n",
    "print(\"✅ Dictionary & Corpus created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95e0dd7-289f-4061-98ef-8a4d498529b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sample Processed Texts:\", processed_texts[:2])  # Show first 2 entries\n",
    "print(\"Dictionary:\", dictionary.token2id)  # Show dictionary mapping\n",
    "print(\"First Corpus Entry:\", corpus[0])  # Show first document in BoW format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208f5aef-148f-46cf-8da2-b19bb22b01cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "# Set number of topics (adjust as needed)\n",
    "num_topics = 5\n",
    "\n",
    "# Train the LDA model\n",
    "lda_model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=num_topics,\n",
    "    random_state=42,\n",
    "    passes=10,\n",
    "    per_word_topics=True\n",
    ")\n",
    "\n",
    "print(\"✅ LDA Model trained successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1407519e-b210-42c0-8398-33582719388e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the topics\n",
    "for idx, topic in lda_model.print_topics(num_words=10):\n",
    "    print(f\"🔹 Topic {idx + 1}: {topic}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da912792-bf47-4f18-8fc2-eed1745da10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "# Enable visualization inside Jupyter Notebook\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# Prepare LDA visualization\n",
    "lda_vis = gensimvis.prepare(lda_model, corpus, dictionary)\n",
    "\n",
    "# Display visualization\n",
    "pyLDAvis.display(lda_vis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d125364c-6fc6-474d-b573-29a03608b235",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
